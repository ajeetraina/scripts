{"name":"Hadoop","tagline":"Setting up Single Node Hadoop Cluster through auitomated script","body":"1.Create a user called Hadoop as shown below:\r\n\r\n$ adduser hadoop\r\n\r\n$ passwd hadoop\r\n\r\nFor this series, we will keep hadoop/ hadoop as credential.\r\n\r\nOnce logged in as Hadoop user, one can verify the userid with the below command:\r\n\r\n$id\r\n\r\nIt will show hadoop as uid and gid which means you are good to go.\r\n\r\nWe are going to use hadoop-1.0.3 version for initial setup.\r\n\r\nDownload http://archive.apache.org/dist/hadoop/core/hadoop-1.0.3/hadoop-1.0.3.tar.gz and place it under /usr/local/ directory.\r\n\r\nGo to /usr/local/ directory.\r\n\r\n$sudo tar xvzf hadoop-1.0.3.tar.gz\r\n\r\n$sudo mv hadoop-1.0.3 hadoop\r\n\r\nHADOOP CONFIGURATION FILES:\r\n\r\n$cd /usr/local/hadoop\r\n\r\n$cd conf\r\n\r\n$vi core-site.xml\r\n\r\n<property>\r\n\r\n<name>hadoop.tmp.dir</name>\r\n\r\n<value>/app/hadoop/tmp</value>\r\n\r\n<description>A base for other temporary directories.</description>\r\n\r\n</property>\r\n\r\n<property>\r\n\r\n<name>fs.default.name</name>\r\n\r\n<value>hdfs://localhost:54310</value>\r\n\r\n<description>The name of the default file system. A URI whose\r\n\r\nscheme and authority determine the FileSystem implementation. The\r\n\r\nuri's scheme determines the config property (fs.SCHEME.impl) naming\r\n\r\nthe FileSystem implementation class. The uri's authority is used to\r\n\r\ndetermine the host, port, etc. for a filesystem.</description>\r\n\r\n</property>\r\n\r\n===================================================\r\n\r\n#vi mapred-site.xml\r\n\r\n<property>\r\n\r\n<name>mapred.job.tracker</name>\r\n\r\n<value>localhost:54311</value>\r\n\r\n<description>The host and port that the MapReduce job tracker runs\r\n\r\nat. If \"local\", then jobs are run in-process as a single map\r\n\r\nand reduce task.\r\n\r\n</description>\r\n\r\n</property>\r\n\r\n==============================================\r\n\r\n#vi hdfs-site.xml\r\n\r\n<property>\r\n\r\n<name>dfs.replication</name>\r\n\r\n<value>1</value>\r\n\r\n<description>Default block replication.\r\n\r\nThe actual number of replications can be specified when the file is created.\r\n\r\nThe default is used if replication is not specified in create time.\r\n\r\n</description>\r\n\r\n</property>\r\n\r\n==============================================================\r\n\r\nThis completes the configuration of Hadoop Configuration files.\r\n\r\nCreate the neccessary directory structure:\r\n\r\n$ cd /usr/local\r\n\r\n$ sudo tar xzf hadoop-1.0.3.tar.gz\r\n\r\n$ sudo mv hadoop-1.0.3 hadoop\r\n\r\n$ sudo chown -R hadoop:hadoop hadoop\r\n\r\n==============================================================\r\n\r\n$cd /usr/local/hadoop/conf\r\n\r\n$sudo vi masters\r\n\r\nAdd this entry only as:\r\n\r\nlocalhost\r\n\r\n$sudo vi slaves\r\n\r\nAdd this entry only as:\r\n\r\nlocalhost\r\n\r\n===============================================\r\n\r\nSetting up JAVA Environment:\r\n\r\nEnsure you are logged in as hadoop user.\r\n\r\n$pwd\r\n\r\n/home/hadoop\r\n\r\n$vi .bashrc\r\n\r\nexport HADOOP_HOME=/usr/local/hadoop\r\n\r\nexport JAVA_HOME=/home/hadoop/software/java/jdk1.6.0_45\r\n\r\nunalias fs &> /dev/null\r\n\r\nalias fs=\"hadoop fs\"\r\n\r\nunalias hls &> /dev/null\r\n\r\nalias hls=\"fs -ls\"\r\n\r\nexport PATH=$PATH:$HADOOP_HOME/bin\r\n\r\nsave the file.\r\n\r\nEnsure the following command is working:\r\n\r\n$bash\r\n\r\n$echo $JAVA_HOME\r\n\r\n$echo $JAVA_HOME\r\n\r\nBoth should show the correct entry as shown in .bashrc\r\n\r\nOpen the file : /usr/local/hadoop/bin/hadoop-env.sh\r\n\r\nMake a one line entry:\r\n\r\nexport JAVA_HOME=/home/hadoop/software/java/jdk1.6.0_45\r\n\r\nsave it.\r\n\r\nRun bash command simply.\r\n\r\nFormatting HDFS:\r\n\r\n#/usr/local/hadoop/bin/hadoop namenode -format\r\n\r\nIt will show like this:\r\n\r\n10/05/08 16:59:56 INFO namenode.NameNode: STARTUP_MSG:\r\n\r\n/************************************************************\r\n\r\nSTARTUP_MSG: Starting NameNode\r\n\r\nSTARTUP_MSG: host = ubuntu/127.0.1.1\r\n\r\nSTARTUP_MSG: args = [-format]\r\n\r\nSTARTUP_MSG: version = 0.20.2\r\n\r\nSTARTUP_MSG: build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-0.20 -r 911707; compiled by 'chrisdo' on Fri Feb 19 08:07:34 UTC 2010\r\n\r\n************************************************************/\r\n\r\n10/05/08 16:59:56 INFO namenode.FSNamesystem: fsOwner=hduser,hadoop\r\n\r\n10/05/08 16:59:56 INFO namenode.FSNamesystem: supergroup=supergroup\r\n\r\n10/05/08 16:59:56 INFO namenode.FSNamesystem: isPermissionEnabled=true\r\n\r\n10/05/08 16:59:56 INFO common.Storage: Image file of size 96 saved in 0 seconds.\r\n\r\n10/05/08 16:59:57 INFO common.Storage: Storage directory .../hadoop-hduser/dfs/name has been successfully formatted.\r\n\r\n10/05/08 16:59:57 INFO namenode.NameNode: SHUTDOWN_MSG:\r\n\r\n/************************************************************\r\n\r\nSHUTDOWN_MSG: Shutting down NameNode at ubuntu/127.0.1.1\r\n\r\n************************************************************\r\n\r\n=================================================================\r\n\r\nEnable Passwordless SSH:\r\n\r\n$ssh-keygen -t rsa -P \"\"\r\n\r\nIt will ask for something and say yes..\r\n\r\n$cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys\r\n\r\n$ssh localhost\r\n\r\nNow you can ssh to localhost machine itself without password.\r\n\r\n====================================================================\r\n\r\nNow lets start the complete Hadoop service through:\r\n\r\n/usr/local/hadoop/bin/start-all.sh\r\n\r\nIt will start namenode and datanode on the same machine..\r\n\r\n====================================================================\r\n\r\nNow lets load the data into Hadoop Cluster.\r\n\r\n$/usr/local/hadoop/bin/hadoop namenode -format\r\n\r\nEsnure you start start-all.sh service before running the below command.\r\n\r\n$sudo /usr/local/hadoop/bin/start-all.sh\r\n\r\nLets copy the data as shown below:\r\n\r\n$cd /usr/local/hadoop\r\n\r\n$sudo bin/hadoop dfs -copyFromLocal /home/hadoop/software/Downloads/1Record /user/hadoop/1\r\n\r\nThis successfully loads a file called 1 into your HDFS filesystem..\r\n\r\nHow to verify?\r\n\r\nRun this command:\r\n\r\n$cd /usr/local/hadoop\r\n\r\n$sudo bin/hadoop dfs -ls /\r\n\r\nIt will show a new directory called /user being created.\r\n\r\nNow Lets analyze the dataset:\r\n\r\n$cd /usr/local/hadoop\r\n\r\n$bin/hadoop jar hadoop-examples-1.0.3.jar wordcount /user/hadoop/1 /user/hduser/1-results\r\n\r\nNow you can open http://<ip>:50030 to see the hadoop jobs\r\n\r\nEnsure you open this link in VM itself as http://localhost:50030.\r\n\r\nif networking works in between host and VM you can successfully open it on host itself.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}